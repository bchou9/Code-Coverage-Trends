{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from GrabReleaseCommits import *\n",
    "from StarHistory import get_star_data, format_dates\n",
    "from GetRepoFromDataset import *\n",
    "from CodeCovReport import get_codecov_all_builds, detect_coverage_tool_usage, get_coverall_all_builds\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from Refined_Data_Set import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_repos = filter_github_repos('../data/github-ranking-2024-02-15.csv')\n",
    "refinedRepo = list()\n",
    "for repo in github_repos:\n",
    "    username, repo_name, token, language = repo[0], repo[1], \"7848dd6f-5308-43f6-a02f-e10e31118854\", repo[2]\n",
    "    repo_info = detect_coverage_tool_usage(\"github\", username, repo_name, token, language)\n",
    "    if repo_info != None:\n",
    "        print(repo_info)\n",
    "        refinedRepo.append(repo_info)\n",
    "\n",
    "print(refinedRepo)\n",
    "print(len(refinedRepo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use refinedData set that is saved\n",
    "Refined_DataSet = final_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data_to_csv(data, csv_filename):\n",
    "    try:\n",
    "        # Load existing data from CSV file\n",
    "        existing_data = pd.read_csv(csv_filename)\n",
    "\n",
    "        # Create a DataFrame from new data\n",
    "        df_new = pd.DataFrame(data, columns=['Username', 'Repository', 'Percentage', 'Hash', 'Timestamp', 'Language', 'Star_List'])\n",
    "\n",
    "        # Fill missing star counts with empty strings\n",
    "        df_new['Star_List'] = df_new['Star_List'].apply(lambda x: x if isinstance(x, list) else ['',''])\n",
    "\n",
    "        # Append the new data to the existing data\n",
    "        combined_data = pd.concat([existing_data, df_new], ignore_index=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        combined_data.to_csv(csv_filename, index=False)\n",
    "\n",
    "        print(f\"New data appended to '{csv_filename}' successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV file not found. Creating a new CSV file...\")\n",
    "        df_new = pd.DataFrame(data, columns=['Username', 'Repository', 'Percentage', 'Hash', 'Timestamp', 'Language', 'Star_List'])\n",
    "\n",
    "        # Fill missing star counts with empty strings\n",
    "        df_new['Star_List'] = df_new['Star_List'].apply(lambda x: x if isinstance(x, list) else ['',''])\n",
    "\n",
    "        df_new.to_csv(csv_filename, index=False)\n",
    "        print(f\"New CSV file '{csv_filename}' created with the new data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_distribution(coverage_list):\n",
    "    if len(coverage_list) <= 50:\n",
    "        return coverage_list\n",
    "    else:\n",
    "        # Generate 10 uniformly spaced indices\n",
    "        uniform_indices = np.linspace(0, len(coverage_list) - 1, 50, dtype=int)\n",
    "        # Retrieve the corresponding elements from the data\n",
    "        return [coverage_list[i] for i in uniform_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "codecov_API_token = \"7848dd6f-5308-43f6-a02f-e10e31118854\"\n",
    "\n",
    "for repo in Refined_DataSet:\n",
    "    username, repo_name, codecov_used, coverall_used, language = repo[1], repo[2], repo[3], repo[4], repo[5]\n",
    "    if codecov_used:\n",
    "        print(f\"CodeCov used {username}/{repo_name}\")\n",
    "        print(f\"https://api.codecov.io/api/v2/github/{username}/repos/{repo_name}/commits/?page=1\")\n",
    "        codecov_report = get_codecov_all_builds('github', username, repo_name, codecov_API_token, language)\n",
    "        if codecov_report == None:\n",
    "            print(\"CodeCov report has a problem\")\n",
    "            continue\n",
    "\n",
    "        reformat_dates_list = [format_dates(dates[4]) for dates in codecov_report]\n",
    "        star_history = get_star_data(username, repo_name, reformat_dates_list)\n",
    "        if star_history == None: #if error occurs in star history then skip the repo\n",
    "            print(\"Star history has a problem\")\n",
    "            continue\n",
    "        for i in range(len(codecov_report)):\n",
    "            if star_history[i][1] != None:\n",
    "                codecov_report[i].append(star_history[i])  \n",
    "        filtered_data = [item for item in codecov_report if any(isinstance(elem, list) for elem in item)]   \n",
    "        uniform_final = uniform_distribution(filtered_data)\n",
    "        # filtered_data = [item for item in uniform_final if any(isinstance(elem, list) for elem in item)]\n",
    "        append_data_to_csv(uniform_final, 'Feb20output.csv')\n",
    "        continue\n",
    "    elif coverall_used:\n",
    "        print(f\"Coverall used {username}/{repo_name}\")\n",
    "        print(f\"https://coveralls.io/github/{username}/{repo_name}.json?page=1\")\n",
    "        coverall = get_coverall_all_builds('github', username, repo_name, language)\n",
    "        reformat_dates_list = [format_dates(dates[4]) for dates in coverall]\n",
    "        star_history = get_star_data(username, repo_name, reformat_dates_list)\n",
    "        if star_history == None:\n",
    "            print(\"Star history has a problem\")\n",
    "            continue\n",
    "        for i in range(len(coverall)):\n",
    "            if star_history[i][1] != None:\n",
    "                coverall[i].append(star_history[i])\n",
    "        filtered_data = [item for item in coverall if any(isinstance(elem, list) for elem in item)]\n",
    "        uniform_final = uniform_distribution(filtered_data)\n",
    "        append_data_to_csv(uniform_final, 'Feb20output.csv')\n",
    "        continue\n",
    "    else:\n",
    "        print(\"No coverage tool used\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook\n",
      "vuejs\n",
      "trekhleb\n",
      "flutter\n",
      "Significant-Gravitas\n",
      "avelino\n",
      "nodejs\n",
      "mui\n",
      "circe\n",
      "Hammerspoon\n",
      "php\n",
      "sorich87\n",
      "apache\n",
      "mozilla-mobile\n",
      "valyala\n",
      "airbnb\n",
      "JuliaDynamics\n",
      "TabbyML\n",
      "GopeedLab\n",
      "alexjoverm\n",
      "cesanta\n",
      "date-fns\n",
      "diem\n",
      "mockk\n",
      "JuliaPy\n",
      "slackapi\n",
      "SciML\n",
      "tqdm\n",
      "pluskid\n",
      "certbot\n",
      "rmanguinho\n",
      "dapr\n",
      "ash-project\n",
      "JohnCoates\n",
      "zio\n",
      "koreader\n",
      "QuantumBFS\n",
      "containers\n",
      "jedisct1\n",
      "best-flutter\n",
      "JuliaCollections\n",
      "starship\n",
      "cube-js\n",
      "grafana\n",
      "wireapp\n",
      "tikv\n",
      "skylot\n",
      "BetterErrors\n",
      "JuliaStats\n",
      "alibaba\n",
      "milvus-io\n",
      "allegro\n",
      "systemd\n",
      "JetBrains\n",
      "bitwarden\n",
      "jump-dev\n",
      "gorilla\n",
      "tsenart\n",
      "MakieOrg\n",
      "JuliaArrays\n",
      "bumptech\n",
      "slackhq\n",
      "iSoron\n",
      "axios\n",
      "PostgREST\n",
      "jogboms\n",
      "YiiGuxing\n",
      "keepassxreboot\n",
      "HeroTransitions\n",
      "finagle\n",
      "swc-project\n",
      "JuliaWeb\n",
      "nightscout\n",
      "tlienart\n",
      "activeadmin\n",
      "fossas\n",
      "railsadminteam\n",
      "LightTable\n",
      "typeorm\n",
      "HangfireIO\n",
      "isar\n",
      "tekartik\n",
      "wez\n",
      "QuantEcon\n",
      "JuliaReinforcementLearning\n",
      "xi-editor\n",
      "pnpm\n",
      "httpie\n",
      "malmaud\n",
      "esp8266\n",
      "mitmproxy\n",
      "pola-rs\n",
      "srs\n",
      "cabol\n",
      "toptal\n",
      "pry\n",
      "kubernetes\n",
      "react-boilerplate\n",
      "abpframework\n",
      "tgstation\n",
      "brettwooldridge\n",
      "firecracker-microvm\n",
      "phoenixframework\n",
      "FluxML\n",
      "JuliaCN\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv('Feb20output1.csv')\n",
    "df = pd.read_csv('../data/test_output.csv')\n",
    "\n",
    "\n",
    "# Get all unique usernames\n",
    "unique_timestamps = df['Username'].unique()\n",
    "\n",
    "for i in unique_timestamps:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
