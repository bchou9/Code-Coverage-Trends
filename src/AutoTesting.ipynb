{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import Repository\n",
    "from collections import defaultdict\n",
    "from GrabReleaseCommits import *\n",
    "from DateUtil import *\n",
    "from StarHistory import get_star_data, get_daily_star_data\n",
    "from GetRepoFromDataset import filter_github_repos\n",
    "from CodeCovReport import get_codecov_all_builds, detect_coverage_tool_usage, get_coverall_all_builds\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import GetPullRequestData\n",
    "# pip install pydriller numpy pandas matplotlib seaborn requests scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_repos = filter_github_repos('../data/Data/github-ranking-2024-02-20_1000.csv')\n",
    "#try except block to ignore any exceptions and prevent them from getting into the way of finishing the loop\n",
    "try:\n",
    "    refinedRepoLow, refinedRepoMed, refinedRepoHigh = list(), list(), list()\n",
    "    for repo in github_repos:\n",
    "        username, repo_name, token, language, stars, forks, issues, last_commit = repo[0], repo[1], \"7848dd6f-5308-43f6-a02f-e10e31118854\", repo[2], repo[3], repo[4], repo[5], repo[6]\n",
    "        repo_info = detect_coverage_tool_usage(\"github\", username, repo_name, token, language)\n",
    "        #Repositories with 10,000 stars or more classified as High Popularity, \n",
    "        #those with star counts ranging from 1,000 to 10,000 as Medium Popularity, \n",
    "        #and repositories with fewer than 1,000 stars were Low Popularity.\n",
    "        if repo_info != None:\n",
    "            if stars < 1000:\n",
    "                print(f'refinedRepoLow: {repo_info} {stars}')\n",
    "                refinedRepoLow.append(repo_info)\n",
    "            elif stars >= 1000 and stars < 10000:\n",
    "                print(f'refinedRepoMed: {repo_info} {stars}')\n",
    "                refinedRepoMed.append(repo_info)\n",
    "            else:\n",
    "                print(f'refinedRepoHigh: {repo_info} {stars}')\n",
    "                refinedRepoHigh.append(repo_info)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting to dict from list of keys and values.\n",
    "def covert_to_dict(listToDictOriginal, columns_in):\n",
    "    listToDictTranspose = list(map(list, zip(*listToDictOriginal))) # to transpose\n",
    "    listToDict = [dict(zip(columns_in, item)) for item in zip(*listToDictTranspose)] # to convert to dict with columns_in as keys\n",
    "    return listToDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import GetPullRequestData as prdata\n",
    "import GlobalVarSetting as globalvar\n",
    "\n",
    "low_len = len(refinedRepoLow)\n",
    "med_len = len(refinedRepoMed)\n",
    "high_len = len(refinedRepoHigh)\n",
    "\n",
    "file_name_low = f'../data/refinedRepoLowStars_{low_len}.csv'\n",
    "file_name_med = f'../data/refinedRepoMedStars_{med_len}.csv'\n",
    "file_name_high = f'../data/refinedRepoHighStars_{high_len}.csv'\n",
    "\n",
    "columns_in = ['platform', 'username', 'repo_name', 'codecov_used', 'coverall_used', 'language']\n",
    "\n",
    "globalvar.write_header_flag = True\n",
    "refinedRepoLowDict = covert_to_dict(refinedRepoLow, columns_in)\n",
    "prdata.append_data_to_csv(refinedRepoLowDict, file_name_low, columns_in)\n",
    "\n",
    "globalvar.write_header_flag = True\n",
    "refinedRepoMedDict = covert_to_dict(refinedRepoMed, columns_in)\n",
    "prdata.append_data_to_csv(refinedRepoMedDict, file_name_med, columns_in)\n",
    "\n",
    "globalvar.write_header_flag = True\n",
    "refinedRepoHighDict = covert_to_dict(refinedRepoHigh, columns_in)\n",
    "prdata.append_data_to_csv(refinedRepoHighDict, file_name_high, columns_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import unique_everseen\n",
    "with open('../data/Refined_Repo_CSV/refinedRepoHighStars_612.csv', 'r') as f, open('../data/Refined_Repo_CSV/refinedRepoHighStars_No_Dupl.csv', 'w') as out_file:\n",
    "    out_file.writelines(unique_everseen(f))\n",
    "    \n",
    "with open('../data/Refined_Repo_CSV/refinedRepoMedStars_1255.csv', 'r') as f, open('../data/Refined_Repo_CSV/refinedRepoMedStars_No_Dupl.csv', 'w') as out_file:\n",
    "    out_file.writelines(unique_everseen(f))\n",
    "    \n",
    "with open('../data/Refined_Repo_CSV/refinedRepoLowStars_119.csv', 'r') as f, open('../data/Refined_Repo_CSV/refinedRepoLowStars_No_Dupl.csv', 'w') as out_file:\n",
    "    out_file.writelines(unique_everseen(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/Refined_Repo_CSV/refinedRepoHighStars_No_Dupl_Henry.csv\")\n",
    "output_csv_path = '../data/Pull_Request_CSV/Pull_Request_History_High_Star_Repo/'\n",
    "\n",
    "username_list = data['username'].tolist()\n",
    "repo_name_list = data['repo_name'].tolist()\n",
    "\n",
    "for username, repo_name in zip(username_list, repo_name_list):\n",
    "    GetPullRequestData.get_pull_requests(output_csv_path, username, repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/Refined_Repo_CSV/refinedRepoMedStars_No_Dupl.csv\")\n",
    "output_csv_path = '../data/Pull_Request_CSV/Pull_Request_History_Med_Star_Repo/'\n",
    "\n",
    "username_list = data['username'].tolist()\n",
    "repo_name_list = data['repo_name'].tolist()\n",
    "\n",
    "for username, repo_name in zip(username_list, repo_name_list):\n",
    "    GetPullRequestData.get_pull_requests(output_csv_path, username, repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/Refined_Repo_CSV/refinedRepoLowStars_No_Dupl.csv\")\n",
    "output_csv_path = '../data/Pull_Request_CSV/Pull_Request_History_Low_Star_Repo/'\n",
    "\n",
    "username_list = data['username'].tolist()\n",
    "repo_name_list = data['repo_name'].tolist()\n",
    "\n",
    "for username, repo_name in zip(username_list, repo_name_list):\n",
    "    GetPullRequestData.get_pull_requests(output_csv_path, username, repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_List(csv_file_path):\n",
    "    # Read CSV data into a DataFrame\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert DataFrame to a list of dictionaries\n",
    "    data_dict = data.to_dict(orient='records')\n",
    "\n",
    "    # Extract username, repository name, and domain, and store in a list\n",
    "    list_of_repos = []\n",
    "    for item in data_dict:\n",
    "        username = item['username']\n",
    "        repo_name = item['repo_name']\n",
    "        codecov = item['codecov_used']\n",
    "        coverall = item['coverall_used']\n",
    "        language = item['language']\n",
    "        list_of_repos.append([username, repo_name, codecov, coverall, language])\n",
    "\n",
    "    return list_of_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data_to_csv(data, csv_filename):\n",
    "    try:\n",
    "        # Load existing data from CSV file\n",
    "        existing_data = pd.read_csv(csv_filename)\n",
    "\n",
    "        # Create a DataFrame from new data\n",
    "        df_new = pd.DataFrame(data, columns=['Username', 'Repository', 'Percentage', 'Hash', 'Timestamp', 'Language', 'Daily Star Count', 'Total Star', 'Additions', 'Deletions'])\n",
    "\n",
    "        # Append the new data to the existing data\n",
    "        combined_data = pd.concat([existing_data, df_new], ignore_index=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        combined_data.to_csv(csv_filename, index=False)\n",
    "\n",
    "        print(f\"New data appended to '{csv_filename}' successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV file not found. Creating a new CSV file...\")\n",
    "        df_new = pd.DataFrame(data, columns=['Username', 'Repository', 'Percentage', 'Hash', 'Timestamp', 'Language', 'Daily Star Count', 'Total Star', 'Additions', 'Deletions'])\n",
    "\n",
    "        df_new.to_csv(csv_filename, index=False)\n",
    "        print(f\"New CSV file '{csv_filename}' created with the new data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_distribution(coverage_list):\n",
    "    if len(coverage_list) <= 50:\n",
    "        return coverage_list\n",
    "    else:\n",
    "        # Generate 10 uniformly spaced indices\n",
    "        uniform_indices = np.linspace(0, len(coverage_list) - 1, 50, dtype=int)\n",
    "        # Retrieve the corresponding elements from the data\n",
    "        return [coverage_list[i] for i in uniform_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_repos = convert_to_List('../data/refinedRepoMedStars_1255.csv')\n",
    "\n",
    "# github_repos = github_repos[534:]\n",
    "# github_repos = github_repos[65:]\n",
    "\n",
    "print(github_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commit_stats(owner, repo, commit_id, access_token):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/commits/{commit_id}\"\n",
    "    headers = {\"Authorization\": f\"token {access_token}\"}\n",
    "    response = requests.get(url, headers=headers, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        commit_data = response.json()\n",
    "        stats = commit_data.get(\"stats\", {})\n",
    "        additions = stats.get(\"additions\", 0)\n",
    "        deletions = stats.get(\"deletions\", 0)\n",
    "        return additions, deletions\n",
    "    else:\n",
    "        print(f\"Failed to fetch commit data: {response.status_code}\")\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#PipeLine\n",
    "codecov_API_token = \"7848dd6f-5308-43f6-a02f-e10e31118854\"\n",
    "api = ['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', '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', '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']\n",
    "current_api_index = 0\n",
    "for repo in github_repos:\n",
    "    print(f\"{github_repos.index(repo) + 1}/{len(github_repos)}\")\n",
    "    api_key = api[current_api_index]\n",
    "    current_api_index = (current_api_index + 1) % len(api) ##added for multiple api keys\n",
    "    username, repo_name, codecov_used, coverall_used, language = repo[0], repo[1], repo[2], repo[3], repo[4]\n",
    "    if codecov_used:\n",
    "        print(f\"CodeCov used {username}/{repo_name}\")\n",
    "        print(f\"https://api.codecov.io/api/v2/github/{username}/repos/{repo_name}/commits/?page=1\")\n",
    "        codecov_report = get_codecov_all_builds('github', username, repo_name, codecov_API_token, language)\n",
    "        if codecov_report == None:\n",
    "            print(\"CodeCov report has a problem\")\n",
    "            continue\n",
    "        if len(codecov_report) <= 5:\n",
    "            print(\"Not enough data. Skipping the repo\")\n",
    "            continue\n",
    "        reformat_dates_list = [format_dates(dates[4]) for dates in codecov_report]\n",
    "        star_history = get_star_data(username, repo_name, reformat_dates_list)\n",
    "        if star_history == None: #if error occurs in star history then skip the repo\n",
    "            print(\"Star history has a problem\")\n",
    "            continue\n",
    "        for i in range(len(codecov_report)):\n",
    "            if star_history[i][1] != None:\n",
    "                codecov_report[i].append(star_history[i][1])  \n",
    "                codecov_report[i].append(star_history[i][2]) \n",
    "        print(\"Finished star history\")\n",
    "        filtered_data = [item for item in codecov_report if len(item) == 8] #8 is the size of the list. Some data don't have code coverage report. So, we need to filter them out.\n",
    "        uniform_final = uniform_distribution(filtered_data)\n",
    "        for i in range(len(uniform_final)):\n",
    "            commit_id = uniform_final[i][3]\n",
    "            additions, deletions = get_commit_stats(username, repo_name, commit_id, api_key)\n",
    "            uniform_final[i].append(additions)\n",
    "            uniform_final[i].append(deletions)\n",
    "        append_data_to_csv(uniform_final, 'MediumMar4Report.csv')\n",
    "        continue\n",
    "    elif coverall_used:\n",
    "        print(f\"Coverall used {username}/{repo_name}\")\n",
    "        print(f\"https://coveralls.io/github/{username}/{repo_name}.json?page=1\")\n",
    "        coverall = get_coverall_all_builds('github', username, repo_name, language)\n",
    "        if len(coverall) <= 5:\n",
    "            print(\"Not enough data. Skipping the repo\")\n",
    "            continue\n",
    "        reformat_dates_list = [format_dates(dates[4]) for dates in coverall]\n",
    "        star_history = get_star_data(username, repo_name, reformat_dates_list)\n",
    "        if star_history == None:\n",
    "            print(\"Star history has a problem\")\n",
    "            continue\n",
    "        for i in range(len(coverall)):\n",
    "            if star_history[i][1] != None:\n",
    "                coverall[i].append(star_history[i][1])\n",
    "                coverall[i].append(star_history[i][2])\n",
    "        print(\"Finished star history\")\n",
    "        filtered_data = [item for item in coverall if len(item) == 8] #8 is the size of the list. Some data don't have code coverage report. So, we need to filter them out.\n",
    "        uniform_final = uniform_distribution(filtered_data)\n",
    "        for i in range(len(uniform_final)):\n",
    "            commit_id = uniform_final[i][3]\n",
    "            additions, deletions = get_commit_stats(username, repo_name, commit_id, api_key)\n",
    "            uniform_final[i].append(additions)\n",
    "            uniform_final[i].append(deletions)\n",
    "        append_data_to_csv(uniform_final, 'MediumMar4Report.csv')\n",
    "        continue\n",
    "    else:\n",
    "        print(\"No coverage tool used\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the parse_date function\n",
    "def parse_date(timestamp):\n",
    "    return datetime.fromtimestamp(timestamp)\n",
    "\n",
    "# Provide your GitHub API token here\n",
    "github_api_token = \"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\"\n",
    "\n",
    "owner = \"grafana\"\n",
    "repo = \"k6\"\n",
    "\n",
    "url = f\"https://api.github.com/repos/{owner}/{repo}/stats/code_frequency\"\n",
    "headers = {\"Authorization\": f\"token {github_api_token}\"}\n",
    "response = requests.get(url, headers=headers, timeout=30)\n",
    "data = response.json()\n",
    "\n",
    "code_frequency_list = []\n",
    "\n",
    "for entry in data:\n",
    "    timestamp = entry[0]\n",
    "    additions = entry[1]\n",
    "    deletions = entry[2]\n",
    "    \n",
    "    # Convert timestamp to datetime using parse_date function\n",
    "    date = parse_date(timestamp)\n",
    "    \n",
    "    entry_dict = {\n",
    "        \"date\": date,\n",
    "        \"additions\": additions,\n",
    "        \"deletions\": deletions\n",
    "    }\n",
    "    \n",
    "    code_frequency_list.append(entry_dict)\n",
    "\n",
    "print(code_frequency_list)\n",
    "\n",
    "formatted_code_frequency_list = []\n",
    "\n",
    "for entry in code_frequency_list:\n",
    "    # Format the datetime object to the desired format\n",
    "    formatted_date = entry[\"date\"].strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    \n",
    "    # Create a new dictionary with the formatted date\n",
    "    formatted_entry = {\n",
    "        \"date\": formatted_date,\n",
    "        \"additions\": entry[\"additions\"],\n",
    "        \"deletions\": entry[\"deletions\"]\n",
    "    }\n",
    "    \n",
    "    formatted_code_frequency_list.append(formatted_entry)\n",
    "\n",
    "print(formatted_code_frequency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add or Removed Lines to CSV\n",
    "access_token = 'access_token_here'\n",
    "\n",
    "# Open the new CSV file in append mode\n",
    "with open(\"updated_test.csv\", 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Read CSV using pandas\n",
    "    df = pd.read_csv(\"../data/FinalReporttest.csv\")\n",
    "\n",
    "    # Iterate over each row and update with commit stats\n",
    "    for index, row in df.iterrows():\n",
    "        owner = row[\"Username\"]\n",
    "        repo = row[\"Repository\"]\n",
    "        commit_id = row[\"Hash\"]\n",
    "        additions, deletions, total = get_commit_stats(owner, repo, commit_id, access_token)\n",
    "        \n",
    "        # Append the commit stats to the row\n",
    "        row[\"Additions\"] = additions\n",
    "        row[\"Deletions\"] = deletions\n",
    "        row[\"Total\"] = total\n",
    "        \n",
    "        # Write the row to the new CSV file\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"CSV file updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for repository javascript: 204\n",
      "Star history has a problem\n",
      "Error fetching data for repository axios: 504\n",
      "Star history has a problem\n",
      "Daily star data updated in the CSV file successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file and extract relevant columns\n",
    "df_old = pd.read_csv(\"../data/FinalReportTemp.csv\")\n",
    "\n",
    "# Add a new column \"Daily Count\" to the DataFrame\n",
    "df_old[\"Daily Count\"] = \"\"\n",
    "\n",
    "# Group timestamps by the combination of Username and Repository\n",
    "grouped_dates = df_old.groupby([\"Username\", \"Repository\"])[\"Timestamp\"].apply(list)\n",
    "\n",
    "# Open the CSV file in append mode\n",
    "with open(\"updated_data.csv\", 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Iterate over each unique combination of Username and Repository\n",
    "    for (username, repo_name), dates in grouped_dates.items():\n",
    "        # Convert timestamps into the required format (\"dd-mm-yyyy\")\n",
    "        formatted_dates = [format_dates(date) for date in dates]\n",
    "\n",
    "        # Fetch daily star data for the combination of Username and Repository\n",
    "        daily_star_data = get_daily_star_data(username, repo_name, formatted_dates)\n",
    "        if daily_star_data == None: #if error occurs in star history then skip the repo\n",
    "            print(\"Star history has a problem\")\n",
    "            continue\n",
    "\n",
    "        # Write the updated data to the CSV file\n",
    "        for data in daily_star_data:\n",
    "            # Write the new row to the CSV file\n",
    "            writer.writerow([username, repo_name] + data)\n",
    "\n",
    "print(\"Daily star data updated in the CSV file successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def format_date(original_timestamp):\n",
    "    try:\n",
    "        # Try parsing with milliseconds\n",
    "        datetime_obj = datetime.strptime(original_timestamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    except ValueError:\n",
    "        # Fallback to parsing without milliseconds\n",
    "        datetime_obj = datetime.strptime(original_timestamp, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    # Convert to the desired format\n",
    "    formatted_date = datetime_obj.strftime(\"%d-%m-%Y\")\n",
    "    return formatted_date\n",
    "\n",
    "# Load data from CSV files\n",
    "add_remove_report = pd.read_csv('../data/AddorRemoveReport.csv')\n",
    "updated_data = pd.read_csv('updated_data.csv')\n",
    "\n",
    "# Apply the format_date function to the 'Timestamp' column in AddorRemoveReport.csv\n",
    "add_remove_report['Timestamp'] = add_remove_report['Timestamp'].apply(format_date)\n",
    "\n",
    "# Merge the two dataframes based on username, repository, and timestamp\n",
    "merged_df = pd.merge(add_remove_report, updated_data, on=['Username', 'Repository'], how='left')\n",
    "\n",
    "# Fill NaN values in 'Daily_Count' column with 0\n",
    "merged_df['Daily_Count'] = merged_df['Daily_Count'].fillna(0)\n",
    "\n",
    "# Convert 'Daily_Count' column to integer\n",
    "merged_df['Daily_Count'] = merged_df['Daily_Count'].astype(int)\n",
    "\n",
    "# Drop duplicate rows\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# Save the merged dataframe back to CSV\n",
    "merged_df.to_csv('merged_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contributor_counts(repo_name,repo_owner,repo_link,filename):    \n",
    "    authors = []\n",
    "    dates = []\n",
    "    for commit in Repository(repo_link).traverse_commits():\n",
    "        authors.append(commit.author.name)\n",
    "        dates.append(commit.author_date.strftime(\"%Y-%m-%d\"))\n",
    "    d = {'authors':authors,'dates':dates}\n",
    "    df = pd.DataFrame(d)\n",
    "    df[\"dates\"] = pd.to_datetime(df[\"dates\"])\n",
    "    df1 = df.groupby([\"dates\",\"authors\"]).size().reset_index(name = \"contributions\")\n",
    "    df2 = df1.groupby([\"dates\"]).size().reset_index(name = \"contributors\")\n",
    "    append_data_to_csv(df2, filename)\n",
    "    print(df2)\n",
    "    \n",
    "def append_data_to_csv(dataframe, csv_filename):\n",
    "    try:\n",
    "        # Load existing data from CSV file\n",
    "        existing_data = pd.read_csv(csv_filename)\n",
    "\n",
    "        # Append the new data to the existing data\n",
    "        combined_data = pd.concat([existing_data, dataframe], ignore_index=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        combined_data.to_csv(csv_filename, index=False)\n",
    "\n",
    "        print(f\"New data appended to '{csv_filename}' successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV file not found. Creating a new CSV file...\")\n",
    "        dataframe.to_csv(csv_filename, index=False)\n",
    "        print(f\"New CSV file '{csv_filename}' created with the new data.\")\n",
    "        \n",
    "def run_contributor_code():\n",
    "    dfhigh = pd.read_csv(\"../data/refinedRepoHighStars_612.csv\")\n",
    "    for i in dfhigh.index:\n",
    "        platform = dfhigh['platform'][i]\n",
    "        username = dfhigh['username'][i]\n",
    "        repo_name = dfhigh['repo_name'][i]\n",
    "        repo_link = f'https://{platform}.com/{username}/{repo_name}.git'\n",
    "        print(repo_link)\n",
    "        filename = f'../data/contributorshigh/{username}_{repo_name}_contributors.csv'\n",
    "        get_contributor_counts(repo_name,username,repo_link,filename)\n",
    "    dfmed = pd.read_csv(\"../data/Refined_Repo_CSV/refinedRepoMedStars_No_Dupl.csv\")\n",
    "    for i in dfmed.index:\n",
    "        platform = dfmed['platform'][i]\n",
    "        username = dfmed['username'][i]\n",
    "        repo_name = dfmed['repo_name'][i]\n",
    "        repo_link = f'https://{platform}.com/{username}/{repo_name}.git'\n",
    "        print(repo_link)\n",
    "        filename = f'../data/contributorsmed/{username}_{repo_name}_contributors.csv'\n",
    "        get_contributor_counts(repo_name,username,repo_link,filename)\n",
    "    dflow = pd.read_csv(\"../data/refinedRepoLowStars_119.csv\")\n",
    "    for i in dflow.index:\n",
    "        platform = dflow['platform'][i]\n",
    "        username = dflow['username'][i]\n",
    "        repo_name = dflow['repo_name'][i]\n",
    "        repo_link = f'https://{platform}.com/{username}/{repo_name}.git'\n",
    "        print(repo_link)\n",
    "        filename = f'../data/contributorslow/{username}_{repo_name}_contributors.csv'\n",
    "        get_contributor_counts(repo_name,username,repo_link,filename)\n",
    "        \n",
    "run_contributor_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data_to_csv(dataframe, csv_filename):\n",
    "    try:\n",
    "        # Load existing data from CSV file\n",
    "        existing_data = pd.read_csv(csv_filename)\n",
    "\n",
    "        # Append the new data to the existing data\n",
    "        combined_data = pd.concat([existing_data, dataframe], ignore_index=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        combined_data.to_csv(csv_filename, index=False)\n",
    "\n",
    "        print(f\"New data appended to '{csv_filename}' successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV file not found. Creating a new CSV file...\")\n",
    "        dataframe.to_csv(csv_filename, index=False)\n",
    "        print(f\"New CSV file '{csv_filename}' created with the new data.\")\n",
    "        \n",
    "def append_contributor_counts():\n",
    "    dfpop = pd.read_csv(\"../data/Popularity_Reports_High_Star/WithPr_HighMar3Report.csv\")\n",
    "    dfpop['contributions'] = ''\n",
    "    prevUser = ''\n",
    "    prevRepo = ''\n",
    "    dftemp = pd.DataFrame()\n",
    "    for i in dfpop.index:\n",
    "        username = dfpop['Username'][i]\n",
    "        repo_name = dfpop['Repository'][i]\n",
    "        filename = f'../data/contributorshigh/{username}_{repo_name}_contributors.csv'\n",
    "        if not (os.path.isfile(filename)):\n",
    "            continue\n",
    "        if not (username == prevUser and repo_name == prevRepo):\n",
    "            dftemp = pd.read_csv(filename)\n",
    "            print(filename)\n",
    "            print(dftemp['dates'])\n",
    "            prevUser = username\n",
    "            prevRepo = repo_name\n",
    "        result = dftemp.loc[dftemp['dates'] == dfpop['Timestamp'][i], 'contributors']\n",
    "        if not (result.size==0):\n",
    "            print(result.values[0])\n",
    "            dfpop.loc[i,'contributions'] = result.values[0]\n",
    "    print(dfpop)\n",
    "    filenamenew = \"../data/Popularity_Reports_High_Star/WithPrandContributions_HighMar3Report.csv\"\n",
    "    append_data_to_csv(dfpop,filenamenew)\n",
    "    dfpop = pd.read_csv(\"../data/Popularity_Reports_Low_Star/WithPr_LowMar5Report.csv\")\n",
    "    dfpop['contributions'] = ''\n",
    "    prevUser = ''\n",
    "    prevRepo = ''\n",
    "    dftemp = pd.DataFrame()\n",
    "    for i in dfpop.index:\n",
    "        username = dfpop['Username'][i]\n",
    "        repo_name = dfpop['Repository'][i]\n",
    "        filename = f'../data/contributorslow/{username}_{repo_name}_contributors.csv'\n",
    "        if not (os.path.isfile(filename)):\n",
    "            continue\n",
    "        if not (username == prevUser and repo_name == prevRepo):\n",
    "            dftemp = pd.read_csv(filename)\n",
    "            print(filename)\n",
    "            print(dftemp['dates'])\n",
    "            prevUser = username\n",
    "            prevRepo = repo_name\n",
    "        result = dftemp.loc[dftemp['dates'] == dfpop['Timestamp'][i], 'contributors']\n",
    "        if not (result.size==0):\n",
    "            print(result.values[0])\n",
    "            dfpop.loc[i,'contributions'] = result.values[0]\n",
    "    print(dfpop)\n",
    "    filenamenew = \"../data/Popularity_Reports_Low_Star/WithPrandContributions_LowMar5Report.csv\"\n",
    "    append_data_to_csv(dfpop,filenamenew)\n",
    "    dfpop = pd.read_csv(\"../data/Popularity_Reports_Medium_Star/WithPr_MediumMar4Report.csv\")\n",
    "    dfpop['contributions'] = ''\n",
    "    prevUser = ''\n",
    "    prevRepo = ''\n",
    "    dftemp = pd.DataFrame()\n",
    "    for i in dfpop.index:\n",
    "        username = dfpop['Username'][i]\n",
    "        repo_name = dfpop['Repository'][i]\n",
    "        filename = f'../data/contributorsmed/{username}_{repo_name}_contributors.csv'\n",
    "        if not (os.path.isfile(filename)):\n",
    "            continue\n",
    "        if not (username == prevUser and repo_name == prevRepo):\n",
    "            dftemp = pd.read_csv(filename)\n",
    "            print(filename)\n",
    "            print(dftemp['dates'])\n",
    "            prevUser = username\n",
    "            prevRepo = repo_name\n",
    "        result = dftemp.loc[dftemp['dates'] == dfpop['Timestamp'][i], 'contributors']\n",
    "        if not (result.size==0):\n",
    "            print(result.values[0])\n",
    "            dfpop.loc[i,'contributions'] = result.values[0]\n",
    "    print(dfpop)\n",
    "    filenamenew = \"../data/Popularity_Reports_Medium_Star/WithPrandContributions_MediumMar4Report.csv\"\n",
    "    append_data_to_csv(dfpop,filenamenew)\n",
    "append_contributor_counts()     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
